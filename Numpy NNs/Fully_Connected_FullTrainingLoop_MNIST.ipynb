{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME+KkOfY+hnoU0+YLhweE4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stewartjollymore/MNIST_NNs/blob/main/Numpy%20NNs/Fully_Connected_FullTrainingLoop_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic371kg4QsA7",
        "outputId": "4fa30140-46d1-4622-bf91-fcc30c0f38a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "#Below we separate and arange the data as seen around the web\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "# encode output which is a number in range [0,9] into a vector of size 10\n",
        "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "# same for test data : 10000 samples\n",
        "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the layer dimensions\n",
        "\n",
        "input_size1 = 28*28\n",
        "output_size1 = 100\n",
        "input_size2 = 100\n",
        "output_size2 = 50\n",
        "input_sizefinal = 50\n",
        "output_sizefinal = 10"
      ],
      "metadata": {
        "id": "aB0h7I0qQ2WM"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#Initialze weights and biases\n",
        "\n",
        "w1 = np.random.rand(input_size1, output_size1) - 0.5\n",
        "b1 = np.random.rand(1, output_size1) - 0.5\n",
        "\n",
        "w2 = np.random.rand(input_size2, output_size2) - 0.5\n",
        "b2 = np.random.rand(1, output_size2) - 0.5\n",
        "\n",
        "w3 = np.random.rand(input_sizefinal, output_sizefinal) -0.5\n",
        "b3 = np.random.rand(1, output_sizefinal) - 0.5"
      ],
      "metadata": {
        "id": "pr996iYPQ-fx"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras as ks\n",
        "\n",
        "#Define activations for forward prop and backprop\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x);\n",
        "\n",
        "def gradient(x):\n",
        "    return 1-np.tanh(x)**2;"
      ],
      "metadata": {
        "id": "FSc5CHHORAwj"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the error metrics\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true-y_pred, 2));\n",
        "\n",
        "def mse_grad(y_true, y_pred):\n",
        "    return 2*(y_pred-y_true)/y_true.size;"
      ],
      "metadata": {
        "id": "vC92eU-tRH0e"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = len(x_train)\n",
        "epochs = 2\n",
        "\n",
        "# training loop\n",
        "for i in range(epochs):\n",
        "  err = 0\n",
        "  for j in range(samples):\n",
        "  # forward propagation\n",
        "    single_xtrain = x_train[j]\n",
        "    single_ytrain = y_train[j]\n",
        "\n",
        "    #Pass through first hidden layer and activation neruon\n",
        "    layer1 = np.dot(single_xtrain,w1)+b1\n",
        "    act_layer1 = tanh(layer1)\n",
        "\n",
        "    #Pass through second hidden layer and activation neuron\n",
        "    layer2 = np.dot(act_layer1,w2)+b2\n",
        "    act_layer2 = tanh(layer2)\n",
        "\n",
        "    #Pass through output layer and acrivation neuron\n",
        "    layer3 = np.dot(act_layer2,w3)+b3\n",
        "    output = tanh(layer3)\n",
        "\n",
        "    err += mse(single_ytrain, output)\n",
        "    #print(single_xtrain)\n",
        "    #Backward Propigation steps\n",
        "\n",
        "    #Define batch size and learning rate\n",
        "    N = single_ytrain.size\n",
        "    learning_rate = 0.01\n",
        "\n",
        "    #calculate the output error\n",
        "    #output_error = mse_grad(single_ytrain,output)\n",
        "    output_error = single_ytrain - output\n",
        "\n",
        "    #Backprop through output layer\n",
        "    output_grad = output_error*gradient(output)\n",
        "    w3_grad = np.dot(act_layer2.T, output_grad)\n",
        "    b3_grad = np.sum(output_grad)\n",
        "    layer2_grad = np.dot(output_grad, w3.T)*gradient(act_layer2)\n",
        "\n",
        "    w2_grad = np.dot(act_layer1.T, layer2_grad)\n",
        "    b2_grad = np.sum(layer2_grad, axis = 0, keepdims=True)\n",
        "    layer1_grad = np.dot(layer2_grad,w2.T)*gradient(act_layer1)\n",
        "\n",
        "    w1_grad = np.dot(single_xtrain.T, layer1_grad)\n",
        "    b1_grad = np.sum(layer1_grad, axis = 0, keepdims = True)\n",
        "\n",
        "    #Updating the new weights by magnitude of the learning rate\n",
        "    w3 += learning_rate*w3_grad\n",
        "    w2 += learning_rate*w2_grad\n",
        "    w1 += learning_rate*w1_grad\n",
        "\n",
        "    #Updating the new biases\n",
        "    b3 += learning_rate*b3_grad\n",
        "    b2 += learning_rate*b2_grad\n",
        "    b1 += learning_rate*b1_grad\n",
        "\n",
        "  err /= samples\n",
        "  print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd7b3njHRLiT",
        "outputId": "8937dd75-adc8-46dc-85fd-538079df9384"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/2   error=0.017335\n",
            "epoch 2/2   error=0.015694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "single_xtest = x_test[0:25]\n",
        "single_ytest = y_test[0:25]\n",
        "\n",
        "#Pass through first hidden layer and activation neruon\n",
        "layer1 = np.dot(single_xtest,w1)+b1\n",
        "act_layer1 = tanh(layer1)\n",
        "\n",
        "#Pass through second hidden layer and activation neuron\n",
        "layer2 = np.dot(act_layer1,w2)+b2\n",
        "act_layer2 = tanh(layer2)\n",
        "\n",
        "#Pass through output layer and acrivation neuron\n",
        "layer3 = np.dot(act_layer2,w3)+b3\n",
        "output = tanh(layer3)\n",
        "\n",
        "from scipy.special import softmax\n",
        "print(mse(single_ytest, output))\n",
        "for i in range(len(output)):\n",
        "  print(\"Predicted:\",np.argmax(output[i], axis = 1), \"Actual:\",np.argmax(single_ytest[i]))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k887gNmhX8Kl",
        "outputId": "65abedb4-75b8-409e-c363-7f9c411b5503"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1671108271558211\n",
            "Predicted: [7] Actual: 7\n",
            "Predicted: [2] Actual: 2\n",
            "Predicted: [1] Actual: 1\n",
            "Predicted: [0] Actual: 0\n",
            "Predicted: [4] Actual: 4\n",
            "Predicted: [1] Actual: 1\n",
            "Predicted: [4] Actual: 4\n",
            "Predicted: [9] Actual: 9\n",
            "Predicted: [5] Actual: 5\n",
            "Predicted: [9] Actual: 9\n",
            "Predicted: [0] Actual: 0\n",
            "Predicted: [6] Actual: 6\n",
            "Predicted: [9] Actual: 9\n",
            "Predicted: [0] Actual: 0\n",
            "Predicted: [1] Actual: 1\n",
            "Predicted: [5] Actual: 5\n",
            "Predicted: [9] Actual: 9\n",
            "Predicted: [7] Actual: 7\n",
            "Predicted: [3] Actual: 3\n",
            "Predicted: [4] Actual: 4\n",
            "Predicted: [9] Actual: 9\n",
            "Predicted: [6] Actual: 6\n",
            "Predicted: [6] Actual: 6\n",
            "Predicted: [5] Actual: 5\n",
            "Predicted: [4] Actual: 4\n"
          ]
        }
      ]
    }
  ]
}