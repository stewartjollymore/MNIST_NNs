{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMygzk0X/bHkym90XiKOTmL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stewartjollymore/MNIST_NNs/blob/main/Numpy%20NNs/Fully_Connected_FullTrainingLoop_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic371kg4QsA7",
        "outputId": "4fa30140-46d1-4622-bf91-fcc30c0f38a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "#Below we separate and arange the data as seen around the web\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "# encode output which is a number in range [0,9] into a vector of size 10\n",
        "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "# same for test data : 10000 samples\n",
        "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the layer dimensions\n",
        "\n",
        "input_size1 = 28*28\n",
        "output_size1 = 100\n",
        "input_size2 = 100\n",
        "output_size2 = 50\n",
        "input_sizefinal = 50\n",
        "output_sizefinal = 10"
      ],
      "metadata": {
        "id": "aB0h7I0qQ2WM"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#Initialze weights and biases\n",
        "\n",
        "w1 = np.random.rand(input_size1, output_size1) - 0.5\n",
        "b1 = np.random.rand(1, output_size1) - 0.5\n",
        "\n",
        "w2 = np.random.rand(input_size2, output_size2) - 0.5\n",
        "b2 = np.random.rand(1, output_size2) - 0.5\n",
        "\n",
        "w3 = np.random.rand(input_sizefinal, output_sizefinal) -0.5\n",
        "b3 = np.random.rand(1, output_sizefinal) - 0.5"
      ],
      "metadata": {
        "id": "pr996iYPQ-fx"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras as ks\n",
        "\n",
        "#Define activations for forward prop and backprop\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x);\n",
        "\n",
        "def gradient(x):\n",
        "    return 1-np.tanh(x)**2;"
      ],
      "metadata": {
        "id": "FSc5CHHORAwj"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the error metrics\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true-y_pred, 2));\n",
        "\n",
        "def mse_grad(y_true, y_pred):\n",
        "    return 2*(y_pred-y_true)/y_true.size;"
      ],
      "metadata": {
        "id": "vC92eU-tRH0e"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = len(x_train)\n",
        "epochs = 2\n",
        "\n",
        "# training loop\n",
        "for i in range(epochs):\n",
        "  err = 0\n",
        "  for j in range(samples):\n",
        "  # forward propagation\n",
        "    single_xtrain = x_train[j]\n",
        "    single_ytrain = y_train[j]\n",
        "\n",
        "    #Pass through first hidden layer and activation neruon\n",
        "    layer1 = np.dot(single_xtrain,w1)+b1\n",
        "    act_layer1 = tanh(layer1)\n",
        "\n",
        "    #Pass through second hidden layer and activation neuron\n",
        "    layer2 = np.dot(act_layer1,w2)+b2\n",
        "    act_layer2 = tanh(layer2)\n",
        "\n",
        "    #Pass through output layer and acrivation neuron\n",
        "    layer3 = np.dot(act_layer2,w3)+b3\n",
        "    output = tanh(layer3)\n",
        "\n",
        "    err += mse(single_ytrain, output)\n",
        "    #print(single_xtrain)\n",
        "    #Backward Propigation steps\n",
        "\n",
        "    #Define batch size and learning rate\n",
        "    N = single_ytrain.size\n",
        "    learning_rate = 0.01\n",
        "\n",
        "    #calculate the output error\n",
        "    #output_error = mse_grad(single_ytrain,output)\n",
        "    output_error = single_ytrain - output\n",
        "\n",
        "    #Backprop through output layer\n",
        "    output_grad = output_error*gradient(output)\n",
        "    w3_grad = np.dot(layer2.T, output_grad)\n",
        "    b3_grad = np.sum(output_grad)\n",
        "    layer2_grad = np.dot(output_grad, w3.T)*gradient(act_layer2)\n",
        "\n",
        "    w2_grad = np.dot(layer1.T, layer2_grad)\n",
        "    b2_grad = np.sum(layer2_grad, axis = 0, keepdims=True)\n",
        "    layer1_grad = np.dot(layer2_grad,w2.T)*gradient(act_layer1)\n",
        "\n",
        "    w1_grad = np.dot(single_xtrain.T, layer1_grad)\n",
        "    b1_grad = np.sum(layer1_grad, axis = 0, keepdims = True)\n",
        "\n",
        "    #Updating the new weights by magnitude of the learning rate\n",
        "    w3 += learning_rate*w3_grad\n",
        "    w2 += learning_rate*w2_grad\n",
        "    w1 += learning_rate*w1_grad\n",
        "\n",
        "    #Updating the new biases\n",
        "    b3 += learning_rate*b3_grad\n",
        "    b2 += learning_rate*b2_grad\n",
        "    b1 += learning_rate*b1_grad\n",
        "\n",
        "  err /= samples\n",
        "  print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd7b3njHRLiT",
        "outputId": "37e60330-646c-4aae-fcb1-edc860e2afed"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/2   error=0.038452\n",
            "epoch 2/2   error=0.032237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "single_xtest = x_test[0:5]\n",
        "single_ytest = y_test[0:5]\n",
        "\n",
        "#Pass through first hidden layer and activation neruon\n",
        "layer1 = np.dot(single_xtest,w1)+b1\n",
        "act_layer1 = tanh(layer1)\n",
        "\n",
        "#Pass through second hidden layer and activation neuron\n",
        "layer2 = np.dot(act_layer1,w2)+b2\n",
        "act_layer2 = tanh(layer2)\n",
        "\n",
        "#Pass through output layer and acrivation neuron\n",
        "layer3 = np.dot(act_layer2,w3)+b3\n",
        "output = tanh(layer3)\n",
        "\n",
        "from scipy.special import softmax\n",
        "print(mse(single_ytest, output))\n",
        "print(output)\n",
        "print(single_ytest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k887gNmhX8Kl",
        "outputId": "cd09f8db-d59c-4921-e619-db2d4b7d0746"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14836153507956196\n",
            "[[[-1.67528267e-02 -4.46503401e-02  3.23200155e-02  1.45708325e-01\n",
            "    4.63883332e-03 -2.19556711e-01  9.36257014e-02  8.27359964e-01\n",
            "   -8.89583267e-02  2.02386661e-01]]\n",
            "\n",
            " [[ 3.05674438e-02 -4.86817108e-02  7.90556598e-01  2.01470452e-01\n",
            "   -1.59790076e-01  1.47472905e-01  1.60399378e-01  5.28219648e-02\n",
            "   -1.75692760e-01 -5.42858498e-04]]\n",
            "\n",
            " [[ 5.12194309e-02  9.05564639e-01 -7.67136727e-02  5.46697093e-02\n",
            "   -6.78935553e-02  8.46016548e-03  9.54258796e-03  1.13365239e-01\n",
            "   -5.59008774e-02 -3.45026420e-04]]\n",
            "\n",
            " [[ 8.94934669e-01 -8.81280318e-02  4.28645983e-02  5.65515742e-02\n",
            "   -6.13193182e-02 -2.87296045e-01  2.50617271e-01 -6.83880883e-02\n",
            "    1.14850392e-01  8.89197868e-02]]\n",
            "\n",
            " [[-1.35603962e-01  1.24824763e-01  1.71282932e-01 -2.88977189e-02\n",
            "    6.11612920e-01 -5.77132729e-03  1.00866207e-01  4.85680896e-03\n",
            "   -1.10478326e-01  2.87065839e-01]]]\n",
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    }
  ]
}